---
marp: true
theme: honwaka
paginate: true
---

<!-- _class: lead -->

# **機械学習講習会**
## **[1] 「学習」**

**2024/06/24** 
**traP Kaggle班**

---

# **はじめに**

---
 


<!-- _header: この講習会のゴール　-->

## ✅ 機械学習の基本的なアイデアを理解して、
## 問題解決の手段として使えるようになる。

---

<!-- _header: スケジュール　-->

## 全7回 + 演習

毎日 20:00~ 

第1回 x/xx モデルと学習
第2回 x/xx 勾配降下法
第3回 x/xx 自動微分
第4回 x/xx ニューラルネットワークの構造
第5回 x/xx ニューラルネットワークの学習と評価
第6回 x/xx PyTorch による実装
〜　　x/xx ミニコンペ

---

<!-- _header: この講習会で扱うこと、扱わないこと　-->

<br>

機械学習は非常に広大な分野 ⇨ 全7回ではちょっと限界がある

今回の講習会ではとくに**ディープラーニング**についてメインに扱います

- ツールを触るだけで原理は全然やらない
- 原理をやるだけで全然使えない

にならないようにどちらもバランス良くやります

---

<!-- _header: 全体の流れ　-->

第1回: 学習
第2回: 勾配降下法
第3回: 自動微分とPyTorch

<br>

第4回: ニューラルネットワークの構造
第5回: ニューラルネットワークの学習と評価
第6回: ニューラルネットワークの実装
<br>


第7回: ニューラルネットワーク発展



---

<!-- _header: 使うプログラミング言語 -->


![bg right height:300](image/python.jpeg)  


### Pythonを使います
自分のPCにインストール or 
Google Colaboratoryなどを利用
<br>


- **第二回までに実行できるようにしておいてください！**


<br>

---

<!-- _header: 使うプログラミング言語や前提知識など -->

### 1.Pythonを使った初歩的なプログラミング
- if文, for文, 関数 など
- 外部パッケージの利用
  
(そこまで高度なことは求めません)

### 2.数学の初歩的な知識
- 基本的な行列の演算や操作 (積、転置など)
- 基本的な微分積分の知識 (偏微分など)
  
(1年前期の (線形代数)　+ (微分積分のさわり) くらい)

---

<!-- _header: 著者のページ -->

![bg right height:300](https://pbs.twimg.com/profile_images/1550352338216398848/Q5nRCd2p_400x400.jpg)

## abap34
- 情報工学系 B2
- Kaggle班班長



---

<!-- _header: 運営を手伝ってくれた人 -->

<br>

内容の議論・チェックなど
- 23B kobakosくん
- 22M YumizSuiさん
- 22M idatenさん
- 22M Silviaseさん

ありがとうございます🙏

---

<!-- _header: 質問・相談など -->

#### ![h:80](image/icon.png) Wuff 　
- テキストを書くとこの画面に流れます
- 匿名なのでお気軽にどうぞ
#### #workshop/machine-learning/sodan
- [traQの相談チャンネル: #event/workshop/machine-learning/sodan](https://q.trap.jp/channels/event/workshop/machine-learning/sodan)　

ありとあらゆるところにお気軽になんでも書いてください！
times に書くときはしれっと「機械学習講習会の...が...」みたいにワードを入れてもらえれば拾います！

---

<!-- _header: 質問・相談など -->

前提として、**大変**

1. 講義パートで引っかかったら秒で書き込みましょう！
(traQでもwuffでも:ok:)
2. 演習パートでは、5分わからなかったら質問してもらって大丈夫です！

もっと大前提として、質問は**迷惑ではないです！**
(むしろ聞いてもらえているということなので、うれしい)

---
<!-- _header: がんばりましょう -->

(ここだけの話、機械学習はめちゃくちゃおもしろい)

<br>
<br>
<br>


# 全7回、がんばりましょう！！



---

<!-- _class: lead-->

# 第一回: 学習

---

<!-- _header: おしながき -->

### 今日の目次

1. 二つの変数の関係を予想しよう ~アイスの売り上げ予測~
2. モデルとパラメータ
3. 「いいモデル」を定義する
   
### 今日の目標

機械学習モデルを構築する上での基本的な用語を整理して、
「学習」ということばをきちんと説明できるようになる。

---

<!-- _header: 大流行、生成AI -->

- ChatGPT ![h:50](image/chatgpt.png)
自然な対話応答

- Stable Diffusion ![h:50](image/sd.png)
お絵描き

- GitHub Copilot ![h:50](image/copilot.Default)
プログラミング

etc...

---



<!-- _header: 機械学習 or AI？　-->

- AI(人工知能)
「人間っぽい知能」を実現しようとする分野・あるいは知能そのもの

- 機械学習(Machine Learning, ML)　
様々な情報から「学習」をして動作するアルゴリズム
人工知能の一つのかたちと見られることが多い 


⬇︎

**機械学習** $\subset$ **人工知能**
( $\leftrightarrow$  スーパーカー $\subset$ 地上をめっちゃ速く走る )

> ここでは一つの定義を紹介しましたが、実際この二つの言葉に明確に定義や合意があるわけではないです。
> なので人工知能か人工知能でないか、機械学習かそうではないか、みたいな議論はやや不毛そうです。


---

<!-- _header: 学習ってなに？　-->


- 機械学習(Machine Learning, ML)　
様々な情報から「**学習**」をして動作するアルゴリズム

<br> 
<br>



↑ **学習**って何？ 


---

<br>

<br>


### 今日のテーマ:

# ✅ 「学習」を説明できるようになる


---

<!-- _header: 「気温」と「アイスの売り上げ」 　-->


- 気温↑　→　売れそう
- 気温↓　→　売れなさそう

「アイスの売り上げ」は「気温」からある程度わかりそう？

![height:30](image/abap34.png)  < 来月の売り上げが予想できたらどのくらい牛乳仕入れたらいいかわかって嬉しいな。


![bg right height:500](image/icecream_scatter.png)


> データは https://okumuralab.org/~okumura/stat/160118.html　から引用
---


<!-- _header: アリスの売り上げを予測する　-->

![height:30](image/abap34.png) < なんか来月の予想平均気温30度って気象庁が言ってたな。

<br>

![height:480](image/icecream_scatter.png)  ![height:30](image/abap34.png) < !!!!!



---

<!-- _header: アリスの売り上げを予測する　-->

![height:550](image/icecream_scatter.png) ![height:30](image/abap34.png) < 過去に30℃のときは...


---

<!-- _header: アリスの売り上げを予測する　-->

一番簡単な方法: 過去の全く同じ状況を参照する

![height:30](image/abap34.png) < ガハハ！これでアイスの売り上げを予測するAIの完成や！



<br>



<br>


![height:60](image/kisyouo.jpg) **<そのまた来月の予想平均気温は40℃です。**

![height:30](image/abap34.png) **< !?**


---


<!-- _header: アリスの売り上げを予測する　-->

![height:550](image/icecream_scatter.png)   ![height:30](image/abap34.png) < 40℃ないやんけ

---


<!-- _header: アリスの売り上げを予測する　-->

「予測」 ... 入力から出力を求める
今回は、「入力: 気温」 → 「出力: アイスの売り上げ」

✅　入力は知ってるものだけとは限らない

![height:50](image/abap34.png) ← こいつが本当にやらなくてはいけなかったことは...

## 売り上げ = $f$(気温)となる関数$f$の推定

このような、入力データを受け取り結果を返す$f$を**モデル**と呼びます。
(厳密に数学的な意味の関数である必要はない)


---


<!-- _header: 線形回帰 -->

売り上げ = $f$(気温)となる関数$f$が、

$f$(気温) = $a \times$気温 + $b$

のかたちで表させると仮定して、$a$と$b$を求める。


例) $a = 20$, $b=100$ なら
アイスの売り上げ $= f$(気温) = $20 \times$気温 + $100$

> $f$が入力変数の線形結合で表せると仮定して、
$y=f(x)$の関係を求める作業を「線形回帰」といいます。

---


<!-- _header: 線形回帰 -->

ためしてみる

![height:500](image/ice_pred_20_100.png) ![height:30](image/abap34.png) < わるくないね



---

<!-- _header: 線形回帰 -->

ためしてみる2 ...  $a = -30$, $b=400$

![height:500](image/ice_pred_-30_400.png) ![height:30](image/abap34.png) < 帰れ


---

<!-- _header: パラメータ -->

$a, b$を変えることでモデルの具体的な形が変わった！

このように各モデルが固有に持ってモデル自身の性質を定める
数を、「**パラメータ**」という。

⬇︎

![height:100](image/do.jpg) は、$f$の構造を決めておけば...
### 「$f$の推定 $\leftrightarrow$ $f$のパラメータの推定」

---

<!-- _header: ちょっとまとめ　-->

- アイスの売り上げを予測するには、気温から売り上げを予測する
  「関数」を構築するのが必要であった。
- 今回は関数の形として $f(x) = ax + b$ (一次関数)を仮定して、
  「関数」を求めることにした。
- この関数は、パラメータとして$a, b$をもち、$a, b$を変えることで
  性質が変わるのがわかった。
- $a,b$を定めることで具体的に関数が定まる。

---

# 休憩します！！


---


<!-- _header: ちょっとまとめ　-->

- アイスの売り上げを予測するには、気温から売り上げを予測する
  「関数」を構築するのが必要であった。
- 今回は関数の形として $f(x) = ax + b$ (一次関数)を仮定して、
  「関数」を求めることにした。
- この関数は、パラメータとして$a, b$をもち、$a, b$を変えることで
  性質が変わるのがわかった。
- $a,b$を定めることで具体的に関数が定まる。

⬇︎

$a, b$を決めよう！


---

<!-- _header: $a, b$の決め方-->

![height:500](image/ice_pred_20_100.png) ![height:30](image/abap34.png) < わるくないね


---

<!-- _header: $a, b$の決め方-->

![height:500](image/ice_pred_25_80.png) ![height:30](image/abap34.png) < ちょっといい？

---

<!-- _header: $a, b$の決め方: 損失の導入 -->

「良さ」とはなにか？

↓

「悪くなさ」

↓

「悪さ」とはなにか？

↓

**「データと予測の遠さ」！**

---

<!-- _header: 二乗話誤差  -->


### 平均二乗誤差(Mean Squared Error)

$$
\dfrac{1}{n}\sum_{i=0}^{n-1} \ (y_i - f(x_i))^2
$$



---

<!-- _header: 損失関数  -->

「悪さ」の指標を「損失関数」という


<br>


### 学習 ... 
### 「損失関数」を最小にする$f$のパラメータを見つける


(どんな複雑な$f$でも共通！！！！)

> 「悪さ」を最小化するのではなく「良さ」を最大化すれば良くない？と思った人もいるかもしれないですね。
> 実はそれはものすごくいい疑問で、次回以降で明らかになっていきます。
> ひとまずは一旦疑問として抱えておいてください。

---


<!-- _header: 注意-->

損失は何の関数？

✅  各$x_i, y_i$は変数みたいな見た目だが、損失関数を考えるときは定数

$$
L(???) = \dfrac{1}{n}\sum_{i=0}^{n-1} \  (y_i - f(x_i))^2
$$

<center>  
<br>
⬇︎
<br>
</center>

$$
L(a, b) = \dfrac{1}{n}\sum_{i=0}^{n-1} \ (y_i - f(x_i; a, b))^2
$$


>(※当然例外もあります)


---

<!-- _header: 今回は...  -->


$L(20, 100) = 40268.55$
![height:450](image/ice_pred_20_100.png) 

---

<!-- _header: 今回は...  -->


$L(25, 80) = 23445.075$
![height:450](image/ice_pred_25_80.png)

✅　こっちの方がよかった！


---

<!-- _header: 今回は...  -->


$L(a, b) = 16482.246499700002$

![height:500](image/ice_pred_36.00780537461502_-126.1282149434462.png)


---

<!-- _header: 当然の疑問 -->

# いや
# それ
# どう
# やったの


---

<!-- _class: lead -->

# $\Large{第二回: 勾配降下法}$
---

<!-- _header: まとめ -->

- アイスの売り上げを予測するには、気温から売り上げを予測する
  「関数」を構築するのが必要であった。
- 今回は関数の形として $f(x) = ax + b$ (一次関数)を仮定して、
  「関数」を求めることにした。
- この関数は、パラメータとして$a, b$をもち、$a, b$を変えることで
  性質が変わるのがわかった。
- $a,b$を定めることで具体的に関数が定まる。
- このパラメータを決める基準として、「悪さ」の基準である損失関数を定めた。
- 損失関数の値を最小化する$a, b$を決めることを「学習」と呼ぶ。

---

<!-- _header: 演習1 -->

1. 平均二乗誤差を計算するpythonのプログラムを実装してください。(不正な入力は来ないという想定でokです)
```python
def mean_squared_error(y, pred):
  # ...
  # ここに書く
  # ...

# アイスの売り上げは実際100個, 200個, 300個だった
y = [100, 200, 300]

# 150個, 220個, 300個と予測
pred = [150, 220, 300]


print('mse:', mean_squared_error(y, pred))
```

---

<!-- _header: 演習2 -->


2. (1.)で実装した平均二乗誤差を使って、損失を$a, b$の関数として書いてください。
```python
def loss(a, b):
  x = [25, 30, 35]
  y = [80, 90, 100]
  # ...
  # ここに書く
  # ...

print('loss:', loss(3, 1))
```

---

<!-- _header: 答え1 -->

```python
def mean_squared_error(y, pred):
    n = len(y)
    s = 0
    for i in range(n):
        s += (y[i] - pred[i])**2
    return s / n

y = [100, 200, 300]
pred = [150, 220, 300]

print('mse:', mean_squared_error(y, pred))
# mse: 966.6666666666666

```

---

<!-- _header: 答え2 -->


```python
def loss(a, b):
    x = [25, 30, 35]
    y = [80, 90, 100]
    pred = []
    for x_i in x:
        pred.append(a * x_i + b)
    return mean_squared_error(y, pred)

print('loss:', loss(3, 1))
# loss: 17.666666666666668
```

```python
# リスト内包表記を使ったバージョン
def loss(a, b):
    x = [25, 30, 35]
    y = [80, 90, 100]
    pred = [a * x_i + b for x_i in x]
    return mean_squared_error(y, pred)
```


